{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spam Filter (Scratch).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-prBWrmKhi3",
        "colab_type": "code",
        "outputId": "59449c81-7c2f-49db-c601-1df059b6ddf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "### This notebook uses Python 3.x ###\n",
        "### Author: GTKlondike            ###\n",
        "\n",
        "\n",
        "# Download data from Github\n",
        "! git clone https://github.com/NetsecExplained/Machine-Learning-for-Security-Analysts.git\n",
        "data_dir = \"Machine-Learning-for-Security-Analysts\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Machine-Learning-for-Security-Analysts' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az-RDxD-JqXz",
        "colab_type": "code",
        "outputId": "23872a8e-b9f3-49e5-b338-4d4eb3ec5ebe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "import re, os, math, nltk, string, json\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "print(\"Libraries imported\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Libraries imported\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0G5Qct1JuGf",
        "colab_type": "code",
        "outputId": "f231bb21-ff03-41af-c69d-87e04794bd16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "test_email = \"\"\"\n",
        "Re: Re: East Asian fonts in Lenny. Thanks for your support.  Installing unifonts did it well for me. ;)\n",
        "Nima\n",
        "--\n",
        "To UNSUBSCRIBE, email to debian-user-REQUEST@lists.debian.org\n",
        "with a subject of \"unsubscribe\". Trouble? Contact listmaster@lists.debian.org\n",
        "\"\"\"\n",
        "print(test_email)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Re: Re: East Asian fonts in Lenny. Thanks for your support.  Installing unifonts did it well for me. ;)\n",
            "Nima\n",
            "--\n",
            "To UNSUBSCRIBE, email to debian-user-REQUEST@lists.debian.org\n",
            "with a subject of \"unsubscribe\". Trouble? Contact listmaster@lists.debian.org\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOjqKKjbJvcz",
        "colab_type": "code",
        "outputId": "fcc0e13c-c098-443e-96ea-5cea94c15918",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "spam_table = dict() # Holds spammy words\n",
        "spam_table_len = 0  # The total number of words in all spam messages\n",
        "total_spam = 0      # The total number of spam emails parsed\n",
        "\n",
        "ham_table = dict()  # Holds hammy words\n",
        "ham_table_len = 0   # The total number of words in all ham messages\n",
        "total_ham = 0       # The total number of ham emails parsed\n",
        "\n",
        "bum_words = \"the;and;that;have;for;not;with;you;this;but;his;from;they;we;say;her;she;will;one;all;would;there;their;what;out;about;who;get;which;when;make;can;like;time;just;him;know;take;people;into;year;your;good;some;could;them;see;other;than;then;now;look;only;come;its;over;think;also;back;after;use;two;how;our;work;first;well;way;even;new;want;because;any;these;give;day;most;ever;among;stand;yet;often;hour;talk;might;start;turn;help;big;small;keep;old;out;high;low;ask;should;down;thing;aaron;adam;alan;albert;alice;amanda;amy;andrea;andrew;angela;ann;anna;anne;annie;anthony;antonio;arthur;ashley;barbara;benjamin;betty;beverly;billy;bobby;bonnie;brandon;brenda;brian;bruce;carl;carlos;carol;carolyn;catherine;charles;cheryl;chris;christina;christine;christopher;clarence;craig;cynthia;daniel;david;deborah;debra;denise;dennis;diana;diane;donald;donna;doris;dorothy;douglas;earl;edward;elizabeth;emily;eric;ernest;eugene;evelyn;frances;frank;fred;gary;george;gerald;gloria;gregory;harold;harry;heather;helen;henry;howard;irene;jack;jacqueline;james;jane;janet;janice;jason;jean;jeffrey;jennifer;jeremy;jerry;jesse;jessica;jimmy;joan;joe;john;johnny;jonathan;jose;joseph;joshua;joyce;juan;judith;judy;julia;julie;justin;karen;katherine;kathleen;kathryn;kathy;keith;kelly;kenneth;kevin;kimberly;larry;laura;lawrence;lillian;linda;lisa;lois;lori;louis;louise;margaret;maria;marie;marilyn;mark;martha;martin;mary;matthew;melissa;michael;michelle;mildred;nancy;nicholas;nicole;norma;pamela;patricia;patrick;paul;paula;peter;philip;phillip;phyllis;rachel;ralph;randy;raymond;rebecca;richard;robert;robin;roger;ronald;rose;roy;ruby;russell;ruth;ryan;samuel;sandra;sara;sarah;scott;sean;sharon;shawn;shirley;stephanie;stephen;steve;steven;susan;tammy;teresa;terry;theresa;thomas;timothy;tina;todd;victor;virginia;walter;wanda;wayne;william;willie\"\n",
        "bum_words = bum_words.split(';')\n",
        "\n",
        "print(\"Initial counters set to 0\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial counters set to 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Brp3CPvjJwgp",
        "colab_type": "code",
        "outputId": "c0277df3-2657-4b3d-a612-dc00b984350b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "def tokenizer(text):\n",
        "    punctuations = list(string.punctuation)\n",
        "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "    stemmer = nltk.stem.PorterStemmer()\n",
        "    # the commented line is for python 2.7\n",
        "    #tokens = nltk.word_tokenize(text.decode('latin1').lower())\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    # Strip out the punctuations\n",
        "    tokens = [i.strip(''.join(punctuations)) \n",
        "              for i in tokens \n",
        "              if i not in punctuations]\n",
        "    # User Porter Stemmer on each token\n",
        "    tokens = [stemmer.stem(i)\n",
        "              for i in tokens]\n",
        "    return [w for w in tokens if w not in stopwords and w != \"\"]\n",
        "\n",
        "t = tokenizer(test_email)\n",
        "print(t)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['east', 'asian', 'font', 'lenni', 'thank', 'support', 'instal', 'unifont', 'well', 'nima', 'unsubscrib', 'email', 'debian-user-request', 'lists.debian.org', 'subject', 'unsubscrib', 'troubl', 'contact', 'listmast', 'lists.debian.org']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUd5gSq-Jx8U",
        "colab_type": "code",
        "outputId": "70f60914-5ea0-4497-ec6e-ddc506ab4b40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "print(str(stopwords)[:100])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'we', 'from', 'in', 'ain', 'now', \"wouldn't\", 'shouldn', 're', 'for', 'wouldn', \"needn't\", 'y', 'it\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIUSUxd3Jy_F",
        "colab_type": "code",
        "outputId": "487516c7-8f0a-4cc3-cbc5-d2433e560895",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "def readEmail(email, method='tokenize'):\n",
        "    table = dict()\n",
        "    word_count = 0\n",
        "    #\n",
        "    if method == 're':\n",
        "        words = re.findall(r'\\b(?:[a-z]{2,}-)*[a-z]{3,}', email.lower())\n",
        "    elif method == 'tokenize':\n",
        "        words = tokenizer(email)\n",
        "    #    \n",
        "    for word in words:\n",
        "        if word not in bum_words:\n",
        "            word_count += 1\n",
        "            if word in table:\n",
        "                table[word] += 1\n",
        "            else:\n",
        "                table[word] = 1\n",
        "    #\n",
        "    return table, word_count\n",
        "\n",
        "t = readEmail(test_email)\n",
        "print(t)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "({'east': 1, 'asian': 1, 'font': 1, 'lenni': 1, 'thank': 1, 'support': 1, 'instal': 1, 'unifont': 1, 'nima': 1, 'unsubscrib': 2, 'email': 1, 'debian-user-request': 1, 'lists.debian.org': 2, 'subject': 1, 'troubl': 1, 'contact': 1, 'listmast': 1}, 19)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlwIwiEUKCBX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def learnSpam(email):\n",
        "    global spam_table\n",
        "    global total_spam\n",
        "    global spam_table_len\n",
        "    \n",
        "    table, word_count = readEmail(email)\n",
        "    \n",
        "    spam_table_len += word_count\n",
        "    total = total_spam\n",
        "    old_spam = spam_table\n",
        "    \n",
        "    for word in old_spam:\n",
        "        if word in table:\n",
        "            # Add to the word count\n",
        "            spam_table[word] += table[word]\n",
        "            \n",
        "            # Quietly delete the word (key pop)\n",
        "            nul = table.pop(word, None)\n",
        "    #\n",
        "    # Add new words to the spam_table\n",
        "    for word in table:\n",
        "        spam_table[word] = table[word]\n",
        "    \n",
        "    # I read a new spam email\n",
        "    total_spam = total + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3zN19mcKCtE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def learnHam(email):\n",
        "    global ham_table\n",
        "    global total_ham\n",
        "    global ham_table_len\n",
        "    \n",
        "    table, word_count = readEmail(email)\n",
        "    \n",
        "    ham_table_len += word_count\n",
        "    total = total_ham\n",
        "    old_ham = ham_table\n",
        "    \n",
        "    for word in old_ham:\n",
        "        if word in table:\n",
        "            # Add to the word count\n",
        "            ham_table[word] += table[word]\n",
        "            \n",
        "            # Quietly delete the word (key pop)\n",
        "            nul = table.pop(word, None)\n",
        "    #\n",
        "    # Add new words to the ham_table\n",
        "    for word in table:\n",
        "        ham_table[word] = table[word]\n",
        "    \n",
        "    # I read a new ham email\n",
        "    total_ham = total + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syIESbQWKFWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "    print(\"training ham\")\n",
        "    for each in os.listdir(data_dir + '/ham'):\n",
        "        with open(data_dir + '/ham/' + each, 'r', encoding='latin-1') as f:\n",
        "            learnHam(f.read())\n",
        "    print(\"training spam\")\n",
        "    for each in os.listdir(data_dir + '/spam'):\n",
        "        with open(data_dir + '/spam/' + each, 'r', encoding='latin-1') as f:\n",
        "            learnSpam(f.read())\n",
        "    print(\"Training complete!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1YCDRT8KHCh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calcN():\n",
        "    # Calculates the total number of unique words\n",
        "    z = spam_table.copy()\n",
        "    z.update(ham_table)\n",
        "    return len(z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67CJ-aBoKIXU",
        "colab_type": "code",
        "outputId": "2aa4aa2c-5bc8-4db4-993d-fcaf9eecad40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# Time to train the tables\n",
        "# Takes a while per training session (5-10 minutes per class)\n",
        "\n",
        "train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training ham\n",
            "training spam\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9gLllQtKJjk",
        "colab_type": "code",
        "outputId": "ee44f97b-2267-48e8-8dac-5cb0bd4c54fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "# Read the contents of the spam_table\n",
        "print(json.dumps(spam_table,indent=4)[:500])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"question\": 122,\n",
            "    \"irelandfrom\": 1,\n",
            "    \"nobodi\": 238,\n",
            "    \"sun\": 257,\n",
            "    \"sep\": 239,\n",
            "    \"18\": 289,\n",
            "    \"20:45:33\": 19,\n",
            "    \"2016\": 231,\n",
            "    \"content-typ\": 262,\n",
            "    \"text/html\": 39,\n",
            "    \"content-transfer-encod\": 239,\n",
            "    \"base64\": 28,\n",
            "    \"pehutuw+dqoncjxirufepg0kpe1fveegahr0cc1lcxvpdj0iq29udgvudc1uexbliibjb250\": 1,\n",
            "    \"zw50psj0zxh0l2h0bww7ignoyxjzzxq9d2luzg93cy0xmjuyij4ncjxnrvrbig5hbwu9ikdf\": 1,\n",
            "    \"tkvsqvrpuiigy29udgvudd0itwljcm9zb2z0iezyb250ugfnzsa0ljaipg0kpe1fveegbmft\": 1,\n",
            "    \"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hovw29LjKKzl",
        "colab_type": "code",
        "outputId": "89000118-4aba-45c9-cfac-e06078456dd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        }
      },
      "source": [
        "# Read the contents of the ham_table\n",
        "print(json.dumps(ham_table,indent=4)[:500])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"ilug\": 302,\n",
            "    \"pppd\": 15,\n",
            "    \"disconnect\": 11,\n",
            "    \"hello\": 88,\n",
            "    \"folk\": 73,\n",
            "    \"linux\": 669,\n",
            "    \"goe\": 88,\n",
            "    \"tri\": 840,\n",
            "    \"connect\": 301,\n",
            "    \"outsid\": 59,\n",
            "    \"world\": 366,\n",
            "    \"modem\": 68,\n",
            "    \"got\": 344,\n",
            "    \"debian\": 685,\n",
            "    \"kernel\": 449,\n",
            "    \"2.4.18\": 5,\n",
            "    \"thi\": 4031,\n",
            "    \"win-modem\": 2,\n",
            "    \"ye\": 256,\n",
            "    \"manag\": 328,\n",
            "    \"locat\": 89,\n",
            "    \"proper\": 42,\n",
            "    \"driver\": 246,\n",
            "    \"minicom\": 6,\n",
            "    \"veri\": 461,\n",
            "    \"much\": 462,\n",
            "    \"abl\": 191,\n",
            "    \"dial\": 7,\n",
            "    \"seem\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Warem4MYKME6",
        "colab_type": "code",
        "outputId": "decc0a5f-4058-4516-cb27-daed255cfbe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "def predict(email, alpha=1, debug=False):\n",
        "    table, word_count = readEmail(email)\n",
        "    \n",
        "    num_h = alpha                             # P(h_theta) = a\n",
        "    num_s = alpha                             # P(s_theta) = a\n",
        "    \n",
        "    N = calcN()\n",
        "    denom_s = spam_table_len + N*alpha        # P(s_theta) = a / [(spam_num) + Na]\n",
        "    denom_h = ham_table_len + N*alpha         # P(h_theata) = a / [(ham_num) + Na]\n",
        "    \n",
        "    # P(x|C)\n",
        "    # Numerator: For each word, that is in the class, count the number\\\n",
        "    #    of occurances in the email. Add together and add 1 (alpha)\n",
        "    # Denominator: The total number of ALL features (words) in a class\n",
        "    #    plus the total number of unique words\n",
        "    #\n",
        "    #                                         # P(s_theta) = (Xi + a) / [(spam_num) + Na]\n",
        "    #                                         # P(h_theta) = (Xi + a) / [(ham_num) + Na]\n",
        "    #\n",
        "    # Now to sum the P(theta)'s\n",
        "    # We are using math.log to prevent overflows\n",
        "    \n",
        "    for word in table:\n",
        "        if word in spam_table:\n",
        "            num_s += math.log(table[word]+1) # add number of times word occurs\n",
        "        if word in ham_table:\n",
        "            num_h += math.log(table[word]+1)\n",
        "    p_spam = float(num_s)/math.log(denom_s)\n",
        "    p_ham = float(num_h)/math.log(denom_h)\n",
        "    \n",
        "    # Which has the greatest probability?\n",
        "    if debug:\n",
        "        print(\"Spam Probability:\", p_spam)\n",
        "        print(\"Ham Probability:\", p_ham)\n",
        "    if (p_spam > p_ham):\n",
        "        ret = 'spam'\n",
        "    else:\n",
        "        ret = 'ham'\n",
        "    return ret\n",
        "\n",
        "# Predict our test email\n",
        "\n",
        "t = predict(test_email, debug=True)\n",
        "print(t)\n",
        "print('')\n",
        "print(test_email)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spam Probability: 0.8259554584442061\n",
            "Ham Probability: 1.051372586916997\n",
            "ham\n",
            "\n",
            "\n",
            "Re: Re: East Asian fonts in Lenny. Thanks for your support.  Installing unifonts did it well for me. ;)\n",
            "Nima\n",
            "--\n",
            "To UNSUBSCRIBE, email to debian-user-REQUEST@lists.debian.org\n",
            "with a subject of \"unsubscribe\". Trouble? Contact listmaster@lists.debian.org\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6HsNF9eKNmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def testModel(alpha=1):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    print(\"reading emails\")\n",
        "    for each in os.listdir(data_dir + '/test'):\n",
        "        with open(data_dir + \"/test/\" + each, 'r', encoding='latin-1') as f:\n",
        "            prediction = predict(f.read(), alpha)\n",
        "            actual = ''.join(x for x in each[-4:] if x.isalpha())\n",
        "            total += 1\n",
        "            if prediction == actual:\n",
        "                correct += 1\n",
        "    print(\"Total Emails: \", total)\n",
        "    print(\"Correctly classified: \", correct)\n",
        "    print(\"Accuracy: \", float(correct)/total)\n",
        "    #return (float(correct)/total)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCCTh3PiKPDY",
        "colab_type": "code",
        "outputId": "e1b214fb-dc32-4add-bd7d-77d58de4102e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "testModel()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading emails\n",
            "Total Emails:  866\n",
            "Correctly classified:  827\n",
            "Accuracy:  0.9549653579676675\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T06PaIN_WnjZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}