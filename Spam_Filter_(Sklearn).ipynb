{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spam Filter (Sklearn).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM57dpKHMi8z",
        "colab_type": "code",
        "outputId": "2b4e8cf5-c6ce-4884-da63-b600b0d09f54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "### This notebook uses Python 3.x ###\n",
        "### Author: GTKlondike            ###\n",
        "\n",
        "\n",
        "# Download data from Github\n",
        "! git clone https://github.com/NetsecExplained/Machine-Learning-for-Security-Analysts.git\n",
        "data_dir = \"Machine-Learning-for-Security-Analysts\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Machine-Learning-for-Security-Analysts'...\n",
            "remote: Enumerating objects: 4134, done.\u001b[K\n",
            "remote: Counting objects: 100% (4134/4134), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4102/4102), done.\u001b[K\n",
            "remote: Total 4134 (delta 30), reused 4114 (delta 23), pack-reused 0\n",
            "Receiving objects: 100% (4134/4134), 14.46 MiB | 14.05 MiB/s, done.\n",
            "Resolving deltas: 100% (30/30), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6QcEtvzX-E9",
        "colab_type": "code",
        "outputId": "981b9844-4fd5-4198-f473-cba29440ae8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import re, os, math, nltk, string, json\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# ======= New Imports =======\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# ======= /New Imports =======\n",
        "\n",
        "print(\"Libraries imported\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Libraries imported\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhkR4nMLMmRT",
        "colab_type": "code",
        "outputId": "d0e09338-f923-408b-9ab3-8c2d6ea9bfbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Straight copy/paste\n",
        "test_email = \"\"\"\n",
        "Re: Re: East Asian fonts in Lenny. Thanks for your support.  Installing unifonts did it well for me. ;)\n",
        "Nima\n",
        "--\n",
        "To UNSUBSCRIBE, email to debian-user-REQUEST@lists.debian.org\n",
        "with a subject of \"unsubscribe\". Trouble? Contact listmaster@lists.debian.org\n",
        "\"\"\"\n",
        "print(test_email)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Re: Re: East Asian fonts in Lenny. Thanks for your support.  Installing unifonts did it well for me. ;)\n",
            "Nima\n",
            "--\n",
            "To UNSUBSCRIBE, email to debian-user-REQUEST@lists.debian.org\n",
            "with a subject of \"unsubscribe\". Trouble? Contact listmaster@lists.debian.org\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJKg9PaNMnA3",
        "colab_type": "code",
        "outputId": "63c21bcc-29aa-4778-ab7b-214abaa26996",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Straight copy/paste\n",
        "def tokenizer(text):\n",
        "    punctuations = list(string.punctuation)\n",
        "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "    stemmer = nltk.stem.PorterStemmer()\n",
        "    # the commented line is for python 2.7\n",
        "    #tokens = nltk.word_tokenize(text.decode('latin1').lower())\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    # Strip out the punctuations\n",
        "    tokens = [i.strip(''.join(punctuations)) \n",
        "              for i in tokens \n",
        "              if i not in punctuations]\n",
        "    # User Porter Stemmer on each token\n",
        "    tokens = [stemmer.stem(i)\n",
        "              for i in tokens]\n",
        "    return [w for w in tokens if w not in stopwords and w != \"\"]\n",
        "\n",
        "t = tokenizer(test_email)\n",
        "print(t)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['east', 'asian', 'font', 'lenni', 'thank', 'support', 'instal', 'unifont', 'well', 'nima', 'unsubscrib', 'email', 'debian-user-request', 'lists.debian.org', 'subject', 'unsubscrib', 'troubl', 'contact', 'listmast', 'lists.debian.org']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tD1gB8EUMn6Q",
        "colab_type": "code",
        "outputId": "137057ce-4690-4bbd-a0bd-f8f859c9f17e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Reworked version of train() function\n",
        "# Takes a while per training session (5-10 minutes per class)\n",
        "\n",
        "corpus = []\n",
        "y = []\n",
        "print(\"Training ham\")\n",
        "for each in os.listdir(data_dir + '/ham'):\n",
        "        with open(data_dir + '/ham/' + each, 'r', encoding='latin-1') as f:\n",
        "            corpus.append(f.read())\n",
        "            y.append(\"ham\")\n",
        "print(\"Training spam\")\n",
        "for each in os.listdir(data_dir + '/spam'):\n",
        "        with open(data_dir + '/spam/'+each, 'r', encoding='latin-1') as f:\n",
        "            corpus.append(f.read())\n",
        "            y.append(\"spam\")\n",
        "print(\"Training Complete!\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training ham\n",
            "Training spam\n",
            "Training Complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mk4n6DvQMowG",
        "colab_type": "code",
        "outputId": "e708a588-d832-452e-df3e-8c950705c384",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print(corpus[5])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Spambayes] spambayes package?\n",
            "    Guido> Why would we care about installing a few extra files, as long as\n",
            "    Guido> they're inside a package?\n",
            "\n",
            "I guess you needn't worry about that.  It just doesn't seem \"clean\" to me.\n",
            "\n",
            "S\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYu609QSMpbf",
        "colab_type": "code",
        "outputId": "18666900-ab20-4291-9e51-5bf8ffac289a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "source": [
        "# Vectorizes the training inputs. Takes about 90 seconds to complete\n",
        "print(\"Vectorizing...\")\n",
        "vectorizer = TfidfVectorizer(tokenizer=tokenizer)#get a vector for each url but use our customized tokenizer\n",
        "countVectorizer = CountVectorizer(tokenizer=tokenizer)\n",
        "X = vectorizer.fit_transform(corpus) #get the X vector\n",
        "count_X = countVectorizer.fit_transform(corpus)\n",
        "print(\"Vectorizing Complete!\")\n",
        "\n",
        "\n",
        "print(\"Printing input\")\n",
        "print(X[0])\n",
        "print(y[:10])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vectorizing...\n",
            "Vectorizing Complete!\n",
            "Printing input\n",
            "  (0, 30378)\t0.16969928153062816\n",
            "  (0, 37449)\t0.05735377143169219\n",
            "  (0, 23130)\t0.13013684858723798\n",
            "  (0, 59276)\t0.09470453015104684\n",
            "  (0, 27547)\t0.06049926027811394\n",
            "  (0, 5761)\t0.0747348427251385\n",
            "  (0, 56049)\t0.05287641957283723\n",
            "  (0, 24819)\t0.11415211743023412\n",
            "  (0, 14756)\t0.08678999959458103\n",
            "  (0, 54336)\t0.06214010348985103\n",
            "  (0, 746)\t0.12024844188400292\n",
            "  (0, 38032)\t0.05630817976401394\n",
            "  (0, 3890)\t0.06244475852447297\n",
            "  (0, 2892)\t0.19032136382434178\n",
            "  (0, 336)\t0.09689786452860939\n",
            "  (0, 38470)\t0.11129035707218164\n",
            "  (0, 36888)\t0.10998529312505823\n",
            "  (0, 61104)\t0.04727050596127583\n",
            "  (0, 15484)\t0.21240239131504268\n",
            "  (0, 61080)\t0.07666906709107443\n",
            "  (0, 16491)\t0.1490742618762242\n",
            "  (0, 10270)\t0.08229426774385218\n",
            "  (0, 9481)\t0.3796707932729973\n",
            "  (0, 25498)\t0.22907369110232906\n",
            "  (0, 45038)\t0.10183151126768646\n",
            "  :\t:\n",
            "  (0, 53193)\t0.20879950143034606\n",
            "  (0, 46026)\t0.12286531830448028\n",
            "  (0, 45111)\t0.12405368171760191\n",
            "  (0, 60269)\t0.06888103659892125\n",
            "  (0, 14749)\t0.13965368475367537\n",
            "  (0, 32224)\t0.07271046348161656\n",
            "  (0, 64544)\t0.08051420622070678\n",
            "  (0, 43936)\t0.15409927849455468\n",
            "  (0, 21531)\t0.1178771931647676\n",
            "  (0, 51144)\t0.07205461172923061\n",
            "  (0, 17795)\t0.13849927545848126\n",
            "  (0, 58326)\t0.17472136078826836\n",
            "  (0, 27121)\t0.04766466747231027\n",
            "  (0, 27942)\t0.10415378053991257\n",
            "  (0, 58125)\t0.09572873502771219\n",
            "  (0, 23088)\t0.04072277682382583\n",
            "  (0, 19881)\t0.0627535942044633\n",
            "  (0, 35615)\t0.11981849060499943\n",
            "  (0, 54136)\t0.05079525000599946\n",
            "  (0, 56935)\t0.05547718102047375\n",
            "  (0, 18093)\t0.052627180624553946\n",
            "  (0, 35576)\t0.05514209720765667\n",
            "  (0, 11235)\t0.057599741606952644\n",
            "  (0, 36253)\t0.19032136382434178\n",
            "  (0, 18801)\t0.07229089785777693\n",
            "['ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMDzmws4MqGn",
        "colab_type": "code",
        "outputId": "0e218754-687d-43b1-c326-ffab7298e573",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Reworked version of testModel()\n",
        "test_corpus = []\n",
        "y_test = []\n",
        "print(\"Reading test set\")\n",
        "for each in os.listdir(data_dir + '/test'):\n",
        "        with open(data_dir + '/test/' + each, 'r', encoding='latin-1') as f:\n",
        "            test_corpus.append(f.read())\n",
        "            label = ''.join(x for x in each[-4:] if x.isalpha())\n",
        "            y_test.append(label)\n",
        "print(\"Test set read complete!\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading test set\n",
            "Test set read complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHNTYCgQMqwY",
        "colab_type": "code",
        "outputId": "2e22fc80-186f-4367-9c1b-1ca12e3a11d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "# Vectorizes the test inputs. Takes about 30 seconds to complete\n",
        "\n",
        "print(\"Vectorizing...\")\n",
        "X_test = vectorizer.transform(test_corpus) #get the X vector\n",
        "count_X_test = countVectorizer.transform(test_corpus)\n",
        "print(\"Vectorizing complete!\")\n",
        "print(\"\")\n",
        "print(X_test[0])\n",
        "print(y_test[:10])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vectorizing...\n",
            "Vectorizing complete!\n",
            "\n",
            "  (0, 57944)\t0.26295947598887165\n",
            "  (0, 57612)\t0.561548532844958\n",
            "  (0, 56874)\t0.280774266422479\n",
            "  (0, 56409)\t0.1848158362029182\n",
            "  (0, 56317)\t0.11220371491036285\n",
            "  (0, 54331)\t0.24246070739206158\n",
            "  (0, 53948)\t0.15070277706082222\n",
            "  (0, 34765)\t0.1833625215713884\n",
            "  (0, 29879)\t0.04759366991605391\n",
            "  (0, 29671)\t0.1364695074879047\n",
            "  (0, 29455)\t0.1561252359799158\n",
            "  (0, 24653)\t0.24656833238037618\n",
            "  (0, 19625)\t0.16027362602762493\n",
            "  (0, 19595)\t0.1061861873671019\n",
            "  (0, 18657)\t0.42582322896345\n",
            "  (0, 13573)\t0.22464591695845426\n",
            "['ham', 'ham', 'ham', 'spam', 'spam', 'ham', 'spam', 'ham', 'ham', 'ham']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X62-VpY3MrUw",
        "colab_type": "code",
        "outputId": "1054a420-a29b-4afe-9bde-2ee9ee5fe230",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# MNB with TF-IDF\n",
        "# === MNB ===\n",
        "\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(X, y)\n",
        "\n",
        "print(\"multiNB predictor\")\n",
        "print(mnb.score(X_test, y_test))\n",
        "mnb_pred = mnb.predict(X_test)\n",
        "print(confusion_matrix(mnb_pred, y_test))\n",
        "\n",
        "# === /MNB ==="
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "multiNB predictor\n",
            "0.8799076212471132\n",
            "[[590 104]\n",
            " [  0 172]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rzTGHNfMsBp",
        "colab_type": "code",
        "outputId": "171137cc-8179-4eed-b528-a1a46b6b0543",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# MNB with Count Vectorizer\n",
        "# === MNB ===\n",
        "\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(count_X, y)\n",
        "\n",
        "print(\"multiNB predictor\")\n",
        "print(mnb.score(count_X_test, y_test))\n",
        "mnb_pred = mnb.predict(count_X_test)\n",
        "print(confusion_matrix(mnb_pred, y_test))\n",
        "\n",
        "# === /MNB ==="
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "multiNB predictor\n",
            "0.9422632794457275\n",
            "[[564  24]\n",
            " [ 26 252]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yoc2efFxMsw0",
        "colab_type": "code",
        "outputId": "65219c3d-e8f5-4581-a52a-4af9f989102f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# LGS with TF-IDF\n",
        "# === /LGS ===\n",
        "\n",
        "lgs = LogisticRegression(solver='lbfgs', max_iter=1000)    #using logistic regression\n",
        "lgs.fit(X, y)\n",
        "\n",
        "print(\"logistic predictor\")\n",
        "print(lgs.score(X_test, y_test)) #pring the score. It comes out to be 98%\n",
        "lgs_pred = lgs.predict(X_test)\n",
        "print(confusion_matrix(lgs_pred, y_test))\n",
        "\n",
        "# === /LGS ==="
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logistic predictor\n",
            "0.9584295612009238\n",
            "[[585  31]\n",
            " [  5 245]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5vj_np2Mtb4",
        "colab_type": "code",
        "outputId": "b55b980c-cc98-4ae1-cf14-1fd9aa2b5e5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# LGS with Count Vectorizer\n",
        "# === /LGS ===\n",
        "\n",
        "lgs = LogisticRegression(solver='lbfgs', max_iter=1000)    #using logistic regression\n",
        "lgs.fit(count_X, y)\n",
        "\n",
        "print(\"logistic predictor\")\n",
        "print(lgs.score(count_X_test, y_test)) #pring the score. It comes out to be 98%\n",
        "lgs_pred = lgs.predict(count_X_test)\n",
        "print(confusion_matrix(lgs_pred, y_test))\n",
        "\n",
        "# === /LGS ==="
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logistic predictor\n",
            "0.9780600461893765\n",
            "[[583  12]\n",
            " [  7 264]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HPn0uJEZ7_H",
        "colab_type": "code",
        "outputId": "b05bfac9-3cb7-425d-8d4e-883741c1cf8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# Enter your email here to predict\n",
        "\n",
        "working_email = test_email\n",
        "#working_email = test_corpus[10]\n",
        "\n",
        "test_email_vector = vectorizer.transform([working_email])\n",
        "test_email_pred = mnb.predict(test_email_vector)\n",
        "\n",
        "print(test_email_pred)\n",
        "print('')\n",
        "print(working_email)\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ham']\n",
            "\n",
            "\n",
            "Re: Re: East Asian fonts in Lenny. Thanks for your support.  Installing unifonts did it well for me. ;)\n",
            "Nima\n",
            "--\n",
            "To UNSUBSCRIBE, email to debian-user-REQUEST@lists.debian.org\n",
            "with a subject of \"unsubscribe\". Trouble? Contact listmaster@lists.debian.org\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyCSFf4X564u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}